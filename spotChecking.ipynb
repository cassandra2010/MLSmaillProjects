{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for classification and regression problems\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import ExtraTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading dataset\n",
    "# def load_dataset():\n",
    "#     x , y = None , None\n",
    "#     return x,y\n",
    "\n",
    "def load_dataset():\n",
    "    return make_classification(n_samples=1000, n_classes=2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear , non linear , ensemble models\n",
    "\n",
    "# dict as a list of different models we want to evaluate\n",
    "\n",
    "# create a dict of standard models to evaluate {name:object}\n",
    "def define_models(models=dict()):\n",
    "    # linear models\n",
    "    models['logistic'] = LogisticRegression()\n",
    "    alpha = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "    for a in alpha:\n",
    "        models['ridge-'+str(a)] = RidgeClassifier(alpha=a)\n",
    "    models['sgd'] = SGDClassifier(max_iter=1000, tol=1e-3)\n",
    "    models['pa'] = PassiveAggressiveClassifier(max_iter=1000, tol=1e-3)\n",
    "    # non-linear models\n",
    "    n_neighbors = range(1, 21)\n",
    "    for k in n_neighbors:\n",
    "        models['knn-'+str(k)] = KNeighborsClassifier(n_neighbors=k)\n",
    "    models['cart'] = DecisionTreeClassifier()\n",
    "    models['extra'] = ExtraTreeClassifier()\n",
    "    models['svml'] = SVC(kernel='linear')\n",
    "    models['svmp'] = SVC(kernel='poly')\n",
    "    c_values = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "    for c in c_values:\n",
    "        models['svmr'+str(c)] = SVC(C=c)\n",
    "    models['bayes'] = GaussianNB()\n",
    "    # ensemble models\n",
    "    n_trees = 100\n",
    "    models['ada'] = AdaBoostClassifier(n_estimators=n_trees)\n",
    "    models['bag'] = BaggingClassifier(n_estimators=n_trees)\n",
    "    models['rf'] = RandomForestClassifier(n_estimators=n_trees)\n",
    "    models['et'] = ExtraTreesClassifier(n_estimators=n_trees)\n",
    "    models['gbm'] = GradientBoostingClassifier(n_estimators=n_trees)\n",
    "    print('Defined %d models' % len(models))\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we may want to transform a data prior to training and testing\n",
    "# for say standarsization, normalisation, feature selection\n",
    "# we do this in a blanket way to all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make pipeline function takes a defined model and returns a pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_pipeline(model):\n",
    "    steps = list()\n",
    "    steps.append(('standardise' , StandardScaler()))\n",
    "    steps.append(('normalize', MinMaxScaler()))\n",
    "    steps.append(('model',model))\n",
    "    pipeline = Pipeline(steps=steps)\n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluating a single model\n",
    "#we use k-fold cross validation here\n",
    "def evaluate_model(x,y,model,folds,metric):\n",
    "    pipeline = make_pipeline(model)\n",
    "    scores = cross_val_score(pipeline,x,y,scoring=metric , cv=folds)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trap the errors and ignore the warnings\n",
    "#for an exception , make the result as none\n",
    "\n",
    "def robust_evaluate_model(x,y,model,folds,metric):\n",
    "    scores = None\n",
    "    try :\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.filterwarnings(\"ignore\")\n",
    "            scores = evaluate_model(x,y,model,folds,metric)\n",
    "    except:\n",
    "        scores = None\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate dict of models {name:object} , return {name:score}\n",
    "def evaluate_models(x,y,models,folds=10,metric='accuracy'):\n",
    "    results = dict()\n",
    "    for name, model in models.items():\n",
    "        scores = evaluate_model(x,y,model,folds,metric)\n",
    "        if scores is not None:\n",
    "            results[name] = scores\n",
    "            mean_score , std_score = mean(scores),std(scores)\n",
    "            print ('>%s: %.3f (+/-%.3f)' % (name, mean_score, std_score))\n",
    "        else :\n",
    "            print ('>%s: error' % name)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print and plot top n results\n",
    "def summarize_results(results , maximize = True,top_n =10):\n",
    "    if len(results)==0:\n",
    "        print ('no results')\n",
    "        return\n",
    "    n = min(top_n,len(results))\n",
    "    mean_scores = [(k,mean(v)) for k,v in results.items()]\n",
    "    mean_scores = sorted(mean_scores,key=lambda x : x[1])\n",
    "    \n",
    "    if maximize:\n",
    "        mean_scores = list(reversed(mean_scores))\n",
    "    names = [x[0] for x in mean_scores[:n]]\n",
    "    scores = [results[x[0]] for]\n",
    "    print()\n",
    "\n",
    "    \n",
    "    for i in range(n):\n",
    "        name = names[i]\n",
    "        mean_score,std_score = mean(results[name]),std(results[name])\n",
    "        print('Rank=%d, Name=%s, Score=%.3f (+/- %.3f)' % (i+1, name, mean_score, std_score))\n",
    "    plt.boxplot(scores,labels = names)\n",
    "    _,labels = pyplot.xticks()\n",
    "    plt.setp(labels,rotation = 90)\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defined 53 models\n",
      ">knn-11: 0.783 (+/-0.041)\n",
      ">knn-10: 0.767 (+/-0.042)\n",
      ">knn-13: 0.789 (+/-0.041)\n",
      ">knn-12: 0.781 (+/-0.045)\n",
      ">knn-15: 0.799 (+/-0.044)\n",
      ">knn-14: 0.787 (+/-0.038)\n",
      ">knn-17: 0.799 (+/-0.027)\n",
      ">knn-16: 0.791 (+/-0.039)\n",
      ">knn-19: 0.801 (+/-0.031)\n",
      ">knn-18: 0.797 (+/-0.030)\n",
      ">ridge-0.9: 0.848 (+/-0.038)\n",
      ">ridge-0.8: 0.848 (+/-0.038)\n",
      ">et: 0.870 (+/-0.029)\n",
      ">ridge-0.1: 0.847 (+/-0.037)\n",
      ">svmr0.7: 0.833 (+/-0.030)\n",
      ">ridge-0.3: 0.847 (+/-0.037)\n",
      ">ridge-0.2: 0.847 (+/-0.037)\n",
      ">ridge-0.5: 0.848 (+/-0.038)\n",
      ">sgd: 0.815 (+/-0.070)\n",
      ">ridge-0.7: 0.848 (+/-0.038)\n",
      ">ridge-0.6: 0.848 (+/-0.038)\n",
      ">svmr1.0: 0.837 (+/-0.032)\n",
      ">bag: 0.860 (+/-0.040)\n",
      ">extra: 0.768 (+/-0.031)\n",
      ">ridge-0.4: 0.848 (+/-0.038)\n",
      ">rf: 0.865 (+/-0.041)\n",
      ">pa: 0.769 (+/-0.076)\n",
      ">ada: 0.850 (+/-0.035)\n",
      ">svmr0.3: 0.805 (+/-0.032)\n",
      ">svmr0.2: 0.781 (+/-0.043)\n",
      ">svmr0.1: 0.797 (+/-0.034)\n",
      ">ridge-1.0: 0.847 (+/-0.038)\n",
      ">svmr0.6: 0.828 (+/-0.031)\n",
      ">svmr0.5: 0.823 (+/-0.029)\n",
      ">svmr0.4: 0.817 (+/-0.032)\n",
      ">svml: 0.843 (+/-0.035)\n",
      ">svmr0.9: 0.838 (+/-0.034)\n",
      ">svmr0.8: 0.838 (+/-0.033)\n",
      ">knn-20: 0.802 (+/-0.034)\n",
      ">cart: 0.797 (+/-0.042)\n",
      ">svmp: 0.773 (+/-0.034)\n",
      ">bayes: 0.816 (+/-0.034)\n",
      ">gbm: 0.866 (+/-0.044)\n",
      ">knn-9: 0.773 (+/-0.044)\n",
      ">knn-8: 0.762 (+/-0.044)\n",
      ">logistic: 0.846 (+/-0.039)\n",
      ">knn-1: 0.726 (+/-0.042)\n",
      ">knn-3: 0.740 (+/-0.038)\n",
      ">knn-2: 0.688 (+/-0.030)\n",
      ">knn-5: 0.767 (+/-0.036)\n",
      ">knn-4: 0.727 (+/-0.027)\n",
      ">knn-7: 0.769 (+/-0.037)\n",
      ">knn-6: 0.757 (+/-0.026)\n",
      "()\n",
      "Rank=1, Name=et, Score=0.870 (+/- 0.029)\n",
      "Rank=2, Name=gbm, Score=0.866 (+/- 0.044)\n",
      "Rank=3, Name=rf, Score=0.865 (+/- 0.041)\n",
      "Rank=4, Name=bag, Score=0.860 (+/- 0.040)\n",
      "Rank=5, Name=ada, Score=0.850 (+/- 0.035)\n",
      "Rank=6, Name=ridge-0.4, Score=0.848 (+/- 0.038)\n",
      "Rank=7, Name=ridge-0.6, Score=0.848 (+/- 0.038)\n",
      "Rank=8, Name=ridge-0.7, Score=0.848 (+/- 0.038)\n",
      "Rank=9, Name=ridge-0.5, Score=0.848 (+/- 0.038)\n",
      "Rank=10, Name=ridge-0.8, Score=0.848 (+/- 0.038)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "global name 'scores' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-166-a99e8d6748a9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefine_models\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_models\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0msummarize_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-165-c7cf3c2ab974>\u001b[0m in \u001b[0;36msummarize_results\u001b[0;34m(results, maximize, top_n)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mmean_score\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstd_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Rank=%d, Name=%s, Score=%.3f (+/- %.3f)'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstd_score\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mboxplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpyplot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxticks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrotation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m90\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: global name 'scores' is not defined"
     ]
    }
   ],
   "source": [
    "x , y = load_dataset()\n",
    "models = define_models()\n",
    "results = evaluate_models(x,y,models)\n",
    "summarize_results(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ii"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
